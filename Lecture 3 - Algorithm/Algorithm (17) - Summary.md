ðŸ“‹ Lecture 3 Summary â€” Algorithms @ CS50

This lesson was about **algorithmic thinking**: how to measure, compare, and choose algorithms based on their efficiency. You also saw how combining data with custom types (structs) and expressing complex logic (recursion) opens the door to dramatically more powerful algorithms.

---

## What You Learned in Lecture 3

---

### 1. ðŸ§­ Algorithms

An **algorithm** is a step-by-step set of instructions for completing a task. The key insight of this lecture: two correct algorithms can solve the same problem with vastly different efficiency. Correctness alone is not enough â€” a good engineer thinks about how *fast* and how *scalable* a solution is.

| Topic covered | Key file |
|--------------|----------|
| What algorithms are | Algorithm (00) |
| Linear search â€” iterate through every element | Algorithm (00), (03), (11) |
| Binary search â€” divide and conquer on sorted arrays | Algorithm (01), (12) |
| Searching through structs | Algorithm (05) |

---

### 2. ðŸ“ˆ Big O Notation

**Big O notation** is the language used to describe how an algorithm's running time grows as the input size grows. Three notations capture different aspects:

| Notation | Meaning | Describes |
|----------|---------|-----------|
| O(n) | Upper bound | Worst case â€” the most steps it could ever take |
| Î©(n) | Lower bound | Best case â€” the fewest steps in the ideal scenario |
| Î˜(n) | Tight bound | Best case = worst case â€” always the same |

**The Big O hierarchy** (fastest â†’ slowest):

```
O(1)  <  O(log n)  <  O(n)  <  O(n log n)  <  O(nÂ²)
```

**Asymptotic simplification rules:**
- Drop constants: O(3n) â†’ O(n)
- Drop lower-order terms: O(nÂ² + n) â†’ O(nÂ²)
- Only the dominant term matters for large n

| Key file |
|----------|
| Algorithm (02) |

---

### 3. ðŸ” Binary Search and Linear Search

| Algorithm | Requirement | Best case | Worst case | How it works |
|-----------|------------|-----------|------------|--------------|
| **Linear search** | None | Î©(1) | O(n) | Check every element left to right |
| **Binary search** | Sorted array | Î©(1) | O(log n) | Repeatedly halve the search area |

- **Linear search** works on any array and stops the moment it finds the target. If the target is absent, it must check every element.
- **Binary search** can eliminate half the remaining elements in one step â€” but only because the array is sorted, making it safe to discard one half entirely.
- The trade-off: sorting costs time upfront, but for repeated searches on large datasets, binary search pays back that cost many times over.

| Key files |
|----------|
| Algorithm (00), (01), (03), (11), (12) |

---

### 4. ðŸ”ƒ Sorting Algorithms

Searching fast requires a sorted array. Three sorting algorithms were covered:

| Algorithm | Best Î© | Worst O | Tight Î˜ | Can detect sorted? | In-place? |
|-----------|--------|---------|---------|-------------------|-----------|
| **Selection sort** | Î©(nÂ²) | O(nÂ²) | Î˜(nÂ²) | âŒ No | âœ… Yes |
| **Bubble sort** | **Î©(n)** | O(nÂ²) | None | âœ… Yes (swap counter) | âœ… Yes |
| **Merge sort** | **Î©(n log n)** | **O(n log n)** | **Î˜(n log n)** | âŒ No | âŒ No |

**Selection sort** â€” find the minimum, move it to the front, repeat. Always Î˜(nÂ²) regardless of input.

**Bubble sort** â€” compare adjacent pairs, swap if out of order. If a full pass has zero swaps, the array is sorted â†’ quit early. Best case Î©(n).

**Merge sort** â€” recursively divide to single elements, then merge back in sorted order. Always Î˜(n log n) â€” faster than the others for large inputs, at the cost of extra memory.

**Concrete scale comparison:**

| n | Selection/Bubble (nÂ²) | Merge sort (n log n) | Speed gain |
|---|----------------------|---------------------|------------|
| 1,000 | ~1,000,000 steps | ~9,966 steps | ~100Ã— faster |
| 1,000,000 | ~10Â¹Â² steps | ~19,931,568 steps | ~50,000Ã— faster |

| Key files |
|----------|
| Algorithm (06), (07), (09), (13), (14), (16) |

---

### 5. ðŸ”„ Recursion

**Recursion** is when a function calls itself as part of its own definition. Every recursive function needs:

| Part | Purpose | Missing it causes |
|------|---------|-------------------|
| **Base case** | Stops the recursion â€” simplest solvable instance | Infinite loop â†’ stack overflow â†’ crash |
| **Recursive case** | Calls itself with a smaller input, progressing toward the base | Never terminates |

**Key examples:**
- **Factorial:** `fact(n) = n Ã— fact(n-1)`, base case `fact(1) = 1`
- **Fibonacci:** two base cases (`fib(1) = 0`, `fib(2) = 1`), one recursive case
- **Collatz:** one base case (`n == 1`), two recursive cases (even vs odd n)

Recursion is not just a stylistic choice â€” it is what makes **divide-and-conquer algorithms** like merge sort expressible cleanly. Without recursion, merge sort's "sort each half" logic would require complex manual stack management.

| Key files |
|----------|
| Algorithm (08), (10), (15) |

---

## Quick Reference â€” All Running Times

| Algorithm | Best Î© | Worst O | Notes |
|-----------|--------|---------|-------|
| Constant time | Î©(1) | O(1) | Same cost regardless of n |
| Linear search | Î©(1) | O(n) | Check every element |
| Binary search | Î©(1) | O(log n) | Requires sorted array |
| Selection sort | Î©(nÂ²) | O(nÂ²) | Always Î˜(nÂ²) |
| Bubble sort | **Î©(n)** | O(nÂ²) | Early exit if no swaps |
| Merge sort | Î©(n log n) | O(n log n) | Always Î˜(n log n); needs extra memory |
