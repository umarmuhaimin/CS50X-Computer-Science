â±ï¸ Running Time / Time Complexity (Lecture 3 â€” Algorithms @ CS50)

When we design or choose an algorithm, one of the most important questions we ask is: **how long does it take to run?** Not in seconds on a specific machine â€” hardware speeds differ and change over time. Instead, computer scientists use a formal notation to describe how an algorithm's time requirement **grows** as the input gets larger. This is called **running time analysis**, and the language we use for it is **Big O notation**.

---

## 1. ğŸ“ˆ The Big O Graph â€” Formalising the Curves

**(Image 1)**

This is the same shape of graph from Lecture 0, but now the curves have **official names** written in Big O notation:

| Curve | Big O notation | Algorithm example |
|-------|---------------|------------------|
| ğŸ”´ Red (steepest) | **O(n)** | Linear search |
| ğŸŸ¡ Yellow (middle) | **O(n/2)** â†’ simplified to **O(n)** | Linear search, two pages at a time |
| ğŸŸ¢ Green (flattest) | **O(logâ‚‚ n)** | Binary search |

### The key rule about Big O: constants are dropped

Notice that O(n) and O(n/2) are treated as the **same class** â€” both are just written as **O(n)**. Why?

Big O is about the **shape of the curve** as the input grows toward infinity â€” not about the exact count of steps. A constant like `1/2` doesn't change the fundamental growth pattern. Whether you do `n` steps or `n/2` steps, if you double the input, you roughly double the time. That's what matters. The constant multiplier becomes negligible for very large `n`.

**Examples of dropping constants:**
| Actual steps | Big O notation |
|-------------|---------------|
| n | O(n) |
| n/2 | O(n) |
| 3n | O(n) |
| n + 100 | O(n) |
| nÂ² + n | O(nÂ²) â€” only the dominant term kept |
| 5 Â· log n | O(log n) |

- **Clarification â€” why drop constants?** Imagine algorithm A takes `n` steps and algorithm B takes `n/2` steps. For n = 1,000,000, A takes 1,000,000 steps and B takes 500,000. That's 2Ã— faster â€” sounds huge. But both are astronomically slower than an O(log n) algorithm that takes ~20 steps. Big O focuses on which **class** an algorithm belongs to, not fine-grained differences within the same class.

---

## 2. ğŸ† The Full Hierarchy of Common Running Times

Listed from **slowest to fastest** (worst to best):

```
O(nÂ²)       â† slowest
O(n log n)
O(n)
O(log n)
O(1)        â† fastest
```

### Each one explained

**O(1) â€” Constant time** âš¡
- The algorithm always takes the **same number of steps**, no matter how large the input.
- Doubling the input does nothing to the running time.
- Example: accessing an array element by index (`arr[3]`) â€” you jump straight to it, no searching.
- Example: checking if a number is even (`n % 2 == 0`) â€” one operation regardless of how big `n` is.

**O(log n) â€” Logarithmic time** ğŸŸ¢
- Time grows very slowly â€” each step cuts the problem in half.
- Doubling the input adds just **one more step**.
- Example: **binary search** â€” with 1,000,000 elements, worst case is ~20 steps. With 2,000,000 elements, worst case is ~21 steps.

**O(n) â€” Linear time** ğŸŸ¡/ğŸ”´
- Time grows proportionally with input size.
- Doubling the input **doubles** the time.
- Example: **linear search** â€” with 1,000,000 elements, worst case is 1,000,000 checks.

**O(n log n) â€” Linearithmic time**
- Slightly worse than linear, but much better than quadratic.
- Example: **merge sort** (which we'll cover later this week) â€” the gold standard for sorting algorithms.
- The `n` comes from having to process every element; the `log n` comes from the divide-and-conquer splitting.

**O(nÂ²) â€” Quadratic time** ğŸ”´ (slowest in this list)
- Time grows with the **square** of the input.
- Doubling the input **quadruples** the time.
- Example: **bubble sort** and **selection sort** (covered later this week).
- For n = 1,000: 1,000,000 steps. For n = 10,000: 100,000,000 steps.
- For large inputs, this becomes impractically slow.

### Side-by-side comparison table

| n (input size) | O(1) | O(log n) | O(n) | O(n log n) | O(nÂ²) |
|----------------|------|---------|------|-----------|-------|
| 1 | 1 | 0 | 1 | 0 | 1 |
| 10 | 1 | ~3 | 10 | ~33 | 100 |
| 100 | 1 | ~7 | 100 | ~664 | 10,000 |
| 1,000 | 1 | ~10 | 1,000 | ~9,966 | 1,000,000 |
| 1,000,000 | 1 | ~20 | 1,000,000 | ~19,931,568 | 1,000,000,000,000 |

- **Clarification:** Look at n = 1,000,000. O(1) = 1 step. O(log n) = 20 steps. O(n) = 1 million steps. O(nÂ²) = **1 trillion steps**. The differences are staggering. This is why choosing the right algorithm matters more than having the fastest hardware.

---

## 3. ğŸ”  The Three Greek Letters â€” O, Î©, and Î˜

Big O alone only tells part of the story. Computer scientists use three related notations to fully describe an algorithm's behaviour:

### O â€” Big O (upper bound / worst case)

Big O describes the **worst-case** running time. It's a ceiling â€” the algorithm will never be *worse* than this.

| Algorithm | O notation | What it means |
|-----------|-----------|---------------|
| Linear search | O(n) | In the worst case, checks every element â€” target is last or absent |
| Binary search | O(log n) | In the worst case, keeps halving until one element is left |

Think of Big O as the answer to: *"How bad can this get?"*

### Î© â€” Big Omega (lower bound / best case)

Omega describes the **best-case** running time. It's a floor â€” the algorithm will never be *better* than this.

| Algorithm | Î© notation | Scenario |
|-----------|-----------|----------|
| Linear search | Î©(1) | Target is the very first element â€” found in 1 step |
| Binary search | Î©(1) | Target is the middle element on the first check â€” found immediately |

Think of Omega as the answer to: *"How good can this get?"*

### Î˜ â€” Big Theta (tight bound / best = worst case)

Theta is used when the **best case and worst case are the same**. This means the algorithm always takes roughly the same amount of time, regardless of the specific input.

**Example:** Counting the number of elements in an array by iterating through it.
- Best case: still have to go through all n elements.
- Worst case: still have to go through all n elements.
- Therefore: Î˜(n) â€” it's always n steps, no matter what.

**Another example:**
- If an algorithm has O(n) and Î©(n), then Î˜(n) â€” the upper and lower bounds meet.
- If an algorithm has O(n) but Î©(1), there is **no** Theta â€” best and worst differ.

### Summary table

| Notation | Symbol | Represents | Question answered |
|----------|--------|-----------|-------------------|
| Big O | O | Upper bound (worst case) | "How bad can it get?" |
| Big Omega | Î© | Lower bound (best case) | "How good can it get?" |
| Big Theta | Î˜ | Tight bound (best = worst) | "Is it always the same?" |

### Applied to algorithms you've seen

| Algorithm | Best case Î© | Worst case O | Theta Î˜? |
|-----------|------------|-------------|---------|
| Linear search | Î©(1) â€” target is first | O(n) â€” target is last or absent | âŒ No â€” they differ |
| Binary search | Î©(1) â€” target is middle | O(log n) â€” target at edge | âŒ No â€” they differ |

---

## 4. ğŸ“ Asymptotic Notation â€” The Big Picture

All three notations (O, Î©, Î˜) fall under the umbrella of **asymptotic notation**.

"Asymptotic" means: *analysing behaviour as the input size grows toward infinity.*

### Why "asymptotic"?

For small inputs, the constant factors and lower-order terms actually matter. A poorly optimised O(n) algorithm might genuinely be slower than a well-optimised O(nÂ²) algorithm for n = 5. But as n gets large â€” hundreds, millions, billions â€” the growth class dominates completely. The asymptotic behaviour is what defines an algorithm's scalability.

### What we ignore in asymptotic analysis

| We ignore | Why |
|-----------|-----|
| Constant multipliers (`3n` vs `n`) | Don't change the growth shape |
| Lower-order terms (`nÂ² + n` â†’ `nÂ²`) | Dominated by the leading term for large n |
| Hardware speed | We want machine-independent analysis |
| Programming language overhead | Same reason â€” pure algorithmic analysis |

### The practical impact

As inputs scale to real-world sizes (millions of web requests, billions of database records), asymptotic behaviour is what determines whether your program finishes in milliseconds, minutes, or never. This is the foundation for everything in algorithm design:

```
A faster computer running O(nÂ²) will eventually lose to
a slower computer running O(n log n), for large enough n.
```

- **Clarification â€” "you will explore these in detail in future courses":** CS50 introduces these ideas at an intuitive level. In later CS courses (Data Structures, Algorithms, Theory of Computation), you'll learn formal mathematical proofs, limits, and more complex notation. For now, focus on understanding the *shapes* and being able to classify algorithms you encounter.

---

## 5. ğŸ“Œ Summary

### The running time hierarchy (slowest â†’ fastest)
| Notation | Name | Growth feel |
|----------|------|-------------|
| O(nÂ²) | Quadratic | Steps explode â€” input Ã— 2 means steps Ã— 4 |
| O(n log n) | Linearithmic | Slightly worse than linear â€” used in good sorting |
| O(n) | Linear | Steps grow with input â€” double input, double steps |
| O(log n) | Logarithmic | Steps barely grow â€” double input, +1 step |
| O(1) | Constant | Steps never change â€” instant regardless of size |

### Three notations
| Symbol | Meaning | Focus |
|--------|---------|-------|
| **O(f(n))** | Upper bound | Worst case â€” ceiling |
| **Î©(f(n))** | Lower bound | Best case â€” floor |
| **Î˜(f(n))** | Tight bound | Best = worst â€” exact characterisation |

### Algorithms covered so far
| Algorithm | O (worst) | Î© (best) | Î˜? |
|-----------|-----------|---------|-----|
| Linear search | O(n) | Î©(1) | No |
| Binary search | O(log n) | Î©(1) | No |

### Key rules for Big O
1. **Drop constants:** O(n/2) â†’ O(n), O(3n) â†’ O(n)
2. **Drop lower-order terms:** O(nÂ² + n) â†’ O(nÂ²)
3. **It's about the shape**, not the exact step count
4. **Worst case only** â€” the ceiling, not the typical or best
